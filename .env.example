# ============================================================================
# SAM Agent Environment Configuration
#
# This file documents all available configuration options.
# Copy to .env and modify for your environment.
#
# Environment Selection:
#   - Local Development: Use stub backends (no external services required)
#   - Production CPU: Use Ollama (CPU), Qdrant (vector DB)
#   - Production GPU: Use Ollama (GPU), Qdrant (GPU-optimized)
# ============================================================================

# ============================================================================
# Compose & Build Configuration
# ============================================================================

# Docker Compose profile (dev, prod-cpu, prod-gpu)
COMPOSE_PROFILE=dev

# Build target for Docker image (final-base for dev/cpu, final-gpu for gpu)
BUILD_TARGET=final-base

# Build arguments for Docker image
GIT_COMMIT=unknown
BUILD_DATE=unknown
INSTALL_WHISPER=false
INSTALL_COQUI=false

# ============================================================================
# Agent API Configuration
# ============================================================================

# Port for Agent API
AGENT_PORT=8000

# ============================================================================
# Tracing & Observability Configuration (OPTIONAL)
#
# Controls observability backend for agent execution tracking.
#
# Options:
#   - noop (default): No tracing, zero overhead
#   - langsmith: Production observability via LangSmith
#   - langtrace (future): Optional support for Langtrace
#
# Note: Tracing is read-only and never affects agent behavior.
#       Failures in tracing are silent and non-fatal.
# ============================================================================

# Tracer backend to use
TRACER_BACKEND=langsmith

# LangSmith configuration (only used if TRACER_BACKEND=langsmith)
LANGSMITH_ENABLED=true
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_PROJECT=sam-agent

# Legacy support (backward compatibility)
# If LANGSMITH_ENABLED=true, overrides TRACER_BACKEND setting
# (Recommended: use TRACER_BACKEND instead)

# ============================================================================
# LLM Backend Configuration (REQUIRED)
#
# Options:
#   - stub: In-memory stub (for testing, no Ollama needed)
#   - local: Ollama (requires Ollama service running)
#   - prod: Production LLM (Ollama)
# ============================================================================

LLM_BACKEND=stub

# If using local/prod, specify Ollama connection
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_PORT=11434

# Model to use (must be pulled in Ollama first)
# Common models: llama2, mistral, phi, neural-chat
OLLAMA_MODEL=phi

# ============================================================================
# Speech-to-Text (STT) Configuration (OPTIONAL)
#
# Enable STT for audio input processing
# ============================================================================

STT_ENABLED=false

# STT Backend (whisper, coqui, local)
STT_BACKEND=whisper

# Whisper Configuration
WHISPER_MODEL=base
WHISPER_DEVICE=cpu

# ============================================================================
# Text-to-Speech (TTS) Configuration (OPTIONAL)
#
# Enable TTS for audio output generation
# ============================================================================

TTS_ENABLED=false

# TTS Backend (coqui, gtts, elevenlabs)
TTS_BACKEND=coqui

# Coqui TTS Configuration
COQUI_MODEL=tts_models/en/ljspeech/glow-tts
COQUI_DEVICE=cpu

# ============================================================================
# Long-Term Memory (LTM) Configuration (OPTIONAL)
#
# Options:
#   - stub: In-memory stub (no persistence)
#   - cognee: Cognee-based memory (Qdrant backend)
#   - qdrant: Direct Qdrant connection
# ============================================================================

LTM_BACKEND=stub

# If using qdrant, specify connection
QDRANT_URL=http://qdrant:6333
QDRANT_PORT=6333
QDRANT_API_KEY=

# Cognee memory configuration
COGNEE_ENABLED=false
COGNEE_GRAPH_TYPE=networkx

# ============================================================================
# Database Configuration (REQUIRED)
#
# SQLite database for agent state
# ============================================================================

DATABASE_PATH=/app/data/memory.db

# ============================================================================
# Python Configuration
# ============================================================================

# Don't buffer output (important for container logging)
PYTHONUNBUFFERED=1

# Don't write .pyc files
PYTHONDONTWRITEBYTECODE=1

# ============================================================================
# Logging Configuration
# ============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# ============================================================================
# Profile-Specific Examples
# ============================================================================

# -------- Local Development (stub backends) --------
# COMPOSE_PROFILE=dev
# LLM_BACKEND=stub
# STT_ENABLED=false
# TTS_ENABLED=false
# LTM_BACKEND=stub
# No external services required, fast startup

# -------- Production CPU (Ollama, no GPU) --------
# COMPOSE_PROFILE=prod-cpu
# LLM_BACKEND=local
# OLLAMA_BASE_URL=http://ollama:11434
# OLLAMA_MODEL=phi
# STT_ENABLED=false
# TTS_ENABLED=false
# LTM_BACKEND=stub
# Requires Ollama service, CPU only

# -------- Production GPU (Ollama + GPU) --------
# COMPOSE_PROFILE=prod-gpu
# LLM_BACKEND=local
# OLLAMA_BASE_URL=http://ollama:11434
# OLLAMA_MODEL=neural-chat
# STT_ENABLED=true
# STT_BACKEND=whisper
# WHISPER_DEVICE=cuda
# TTS_ENABLED=true
# TTS_BACKEND=coqui
# COQUI_DEVICE=cuda
# LTM_BACKEND=cognee
# Requires NVIDIA GPU, CUDA 12.1+
